# Goal

Simple GAN to generate 'fake' digits that minic the MNIST dataset.

Note: tutorial states that they train for 100 epochs.

# To do

* It seems like after I do some training, the generated image only updates on the *second* call. Is that true?
* Can I disconnect from a running kernel and reconnect to it?
* What's the ratio of the time taken to do a training batch on my laptop vs on p2.xlarge? What's the optimum batch size?
